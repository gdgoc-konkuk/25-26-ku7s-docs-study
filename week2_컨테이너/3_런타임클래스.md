# RuntimeClass

정의: 쿠버네티스가 **컨테이너를 어떤 런타임 환경에서 실행할지 제어하는 메커니즘**에 대한 개념이다.

(이미지를 실행하기 위해 어떤 런타임 엔진을 사용할지 결장하는 개념이다. )

→ 쿠버네티스는 컨테이너를 실행하기 위해 여러 런타임을 사용할 수 있는데, 서로 다른 런타임 환경(ex. 일반 컨테이너 vs 가상화 샌드박스)을 동시에 관라하기 위해 `RuntimeClass` 가 존재한다.

## ⚙️ 동작 구조

**Pod:** `runtimeClassName` 필드로 어떤 런타임을 사용할지 지정

**RuntimeClass 객체:** 클러스터 관리자가 미리 정의한 런타임 설정을 자기 이름으로 관리한다.
                                      또한, 유효한 DNS 레이블 이름이어야 한다. 

**Handler:** kublet이 컨테이너 런타임에게 어떤 프로파일을 사용할지 알려주는 키

```yaml
# 런타임클래스는 node.k8s.io API 그룹에 정의되어 있음
apiVersion: node.k8s.io/v1   
kind: RuntimeClass
metadata:
  name: gvisor          # Pod에서 참조할 이름
handler: runsc          # CRI에 정의된 런타입 핸들러
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sandboxed-pod
spec:
  runtimeClassName: gvisor
  containers:
    - name: app
      image: busybox
      command: ["echo", "Hello from gVisor"]
```

→ 이 Pod는 `gVisor` 라는 샌드박스형 런타임에서 실행된다. 

1. 사용자가 Pod를 정의할 때, `spec`안에  `runtimeclassName` 을 지정한다.
2. Pod가 생성 요청되면 → API 서버는 클러스터 안에 있는 `RuntimeClass` 리소스를 조회한다.
    
    `metadata.name`이 사용자가 지정한 이름의 RuntimeClass를 찾고 그 안의 `handler` 값을 확인한다. ``
    
3. 스케줄러가 Pod를 어떤 node에 배치할지 결정한다.
    
    `nodeSelector`와 `tolerations`를 고려하여 “해당 런타임을 사용할 수 있는 node”만 선택한다.
    
4. 스케줄러가 선택한 node의 kublet이 Pod 생성 요청을 받아 실행한다.
    
    `runtimeClassName`을 확인하고 `handler` 정보를 가져온다. 그 다음, 자신이 사용하는 **CRI** 클라이언트를 통해 지정된 handler로 컨테이너를 실행한다.
    
5. 실제 컨테이너 런타임이 지정된 격리 환경에서 컨테이너를 실행한다. 

## 🧱 런타임 클래스 주요 필드

| 필드명 | 설명 |
| --- | --- |
| **metadata.name** | RuntimeClass의 이름 — Pod의 `runtimeClassName`과 매칭됨 |
| **handler** | Kubelet이 사용할 컨테이너 런타임 핸들러 이름 (예: `runc`, `kata`, `runsc`) |
| **overhead** *(선택)* | 해당 런타임이 사용하는 추가 리소스(메모리, CPU 등) 정의 |
| **scheduling** *(선택)* | 특정 노드에만 스케줄링이 되도록 제약 조건 설정 (예: 전용 샌드박스 노드) |

### (1) Overhead 예시

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: kata
handler: kata-qemu
overhead:              # 런타입 자체가 유발하는 추가 CPU/메모리 오버헤드를 선언
  podFixed:
    cpu: "250m"
    memory: "128Mi"
```

→ `kate` 런타임을 사용하는 Pod마다 **추가로 “0.25 CPU”, “128MIB” 메모모리를 예약**한다. 

### (2) Scheduling 예시

```yaml
scheduling:
  nodeSelector:            # Pod의 nodeSelector와 교집합으로 병합
    runtime: kata 
  tolerations:             # Pod의 tolerations과 합집합으로 병행
    - key: "kata"
      operator: "Exists"
      effect: "NoSchedule"
```

→ 이 런타임은 `runtime=kata` 라벨이 붙은 노드에서만 실행되도록 제한한다.

→ 특정 런타임용 노드를 분리할 때 유용하다.

`nodeSelector`: Pod가 어떤 라벨을 가진 노드에서만 실행될지 지정하는 제약 조건

`tolerations`: 노드의 taint를 무시하고 스케줄이 가능하도록 허용하는 설정

* **taint:** 특정 노드에 오염 표시를 붙여, 아무 Pod나 해당 node에서 스케줄되지 못하게 막는 제약 조건

- 교집합 → nodeSelector의 결합방식
    
    RuntimeClass와 Pod에 nodesSelector이 있을 때, **두 조건 모두 만족하는 노드에서만 실행**이 가능하다.
    
    ```yaml
    # RuntimeClass.scheduling
    scheduling:
      nodeSelector:
        runtime: kata
    
    # Pod.spec
    spec:
      nodeSelector:
        zone: asia-east1
    ```
    
    → Pod는 노드 라벨이 `runtime=kata` & `zon=asia-east1`을 모두 충족해야만 스케줄이 가능하다는 뜻
    
- 합집합 → tolerations의 결합방식
    
    tolerations은 Pod이 **어떤 taint를 가진 노드를 허용할 수 있는지**를 나타낸다.
    
    ```yaml
    # RuntimeClass.scheduling
    scheduling:
      tolerations:
      - key: "runtime"
        operator: "Exists"
        effect: "NoSchedule"
    
    # Pod.spec
    spec:
      tolerations:
      - key: "zone"
        operator: "Equal"
        value: "asia-east1"
        effect: "NoSchedule"
    ```
    
    → Pod와 RuntimeClass **둘 중 하나라도 해당 taint를 허용하면** 그 노드로 스케줄링이 가능하다.
    

### (3) 지원되는 런타임 예시

| 런타임 | 설명 | 격리 수준 |
| --- | --- | --- |
| **runc** | 표준 리눅스 컨테이너 런타임 (기본값) | 일반 |
| **gVisor** | Google이 만든 사용자 공간 샌드박스 런타임 | 높음 |
| **Kata Containers** | 경량 VM 기반 런타임 | 매우 높음 |

→ 즉, `RuntimeClass`를 이용하면 **Pod 단위로 격리 수준을 다르게 적용**할 수 있다.

## 🧠 런타임 클래스가 필요한 이유

| 상황 | RuntimeClass 활용 예시 |
| --- | --- |
| **보안 강화** | `gVisor`, `Kata Containers` 같은 샌드박스 런타임을 지정해서 격리 강화 |
| **성능 최적화** | GPU / 고성능 노드에는 `runc` 런타임 사용, 일반 노드에는 경량 런타임 사용 |
| **테스트 / 개발 분리** | 개발용 Pod은 `docker`, 운영용 Pod은 `containerd`로 분리 가능 |
| **하이브리드 인프라** | 클러스터 내 일부 노드만 특수 런타임 지원 시 스케줄링 제약으로 제어 |

##