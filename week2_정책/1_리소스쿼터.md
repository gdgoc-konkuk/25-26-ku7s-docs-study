한 네임스페이스가 클러스터 전체 자원 중 얼마나 쓸 수 있는지를 통제하는 기능

- LimitRange는 개별 pod/컨테이너 수준의 제한이고 리소스쿼터는 네임스페이스 전체에 대한 총량 제한
- 왜?
  하나의 쿠버네티스 클러스터의 여러 팀이 함께 서비스를 띄울 수 있기 때문에 네임스페이스에 사용 가능 자원을 할당해 관리

### 리소스 종류

리소스쿼터가 관리할 수 있는 리소스 유형

| 리소스 유형               | 설명                                                 |
| ------------------------- | ---------------------------------------------------- |
| **Compute (CPU, Memory)** | 파드들의 CPU·메모리 총합 제한                        |
| **Storage**               | PVC(스토리지 요청) 총합 제한                         |
| **Object Count**          | 생성 가능한 리소스 개수 제한 (Pod 수, Service 수 등) |
| **ConfigMap, Secret 등**  | 네임스페이스 안에서 생성 가능한 객체 수도 제한 가능  |

- 클러스터의 총 용량보다 네임스페이스별 쿼터들의 합이 클 경우 Resource contention이 발생 → 선착순으로 처리
- 이미 실행 중인 파드나 서비스는 새로 변경되거나 생성된 쿼터의 영향을 받지 않음

### 동작 프로세스

1. 리소스쿼터 생성

   각 네임스페이스에 ResourceQuota 오브젝트를 연결

   ```yaml
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: team-a-quota
     namespace: team-a
   spec:
     hard:
       requests.cpu: "10"
       requests.memory: 20Gi
       limits.cpu: "12"
       limits.memory: 24Gi
       pods: "30"
       services: "10"
   ```

2. 사용자가 리소스 생성 시도

   사용자가 Pod나 Service 생성 시도 시, 현재 네임스페이스의 사용량을 추적해 새로운 리소스를 추가했을 때 제한된 한도를 넘는지 계산

3. 한도 초과 시 거부

   API 서버가 HTTP 403 Forbidden과 함께 에러 메시지를 반환

### 주의할 점

CPU, 메모리 제한이 걸려있을 경우 , 모든 새 Pod에 반드시 requests또는 limit 값을 명시해야 함 → 계산 가능

→ LimitRange를 함께 사용해 기본적인 request/limit 값을 사용가능하도록 함

### 예시

| 시나리오                 | 정책 예시                                                                                      |
| ------------------------ | ---------------------------------------------------------------------------------------------- |
| **A팀 vs B팀 자원 분배** | 클러스터 용량: 32Gi RAM, 16코어 → A팀 20Gi·10코어 / B팀 10Gi·4코어 / 나머지 2Gi·2코어는 여유분 |
| **환경별 차등 제한**     | `testing` 네임스페이스: 1코어, 1Gi 제한 / `production`: 제한 없음                              |

### 리소스쿼터 활성화

`--enable-admission-plugins=`의 인수로 `ResoruceQuota`가 있을 경우 활성화

## 컴퓨트 리소스쿼터

CPU, 메모리 같은 연산 자원 사용량의 총합을 제한하는 기능

- 이미 종료된 파드는 계산에 포함하지 않음

| 리소스 이름         | 의미                                                                           |
| ------------------- | ------------------------------------------------------------------------------ |
| **limits.cpu**      | 네임스페이스 내 모든 **파드의 CPU 제한(limit)** 값의 합이 이 값을 넘으면 안 됨 |
| **limits.memory**   | 모든 **파드의 메모리 제한(limit)** 값의 합이 이 값을 넘으면 안 됨              |
| **requests.cpu**    | 모든 **파드의 CPU 요청(request)** 값의 합이 이 값을 넘으면 안 됨               |
| **requests.memory** | 모든 **파드의 메모리 요청(request)** 값의 합이 이 값을 넘으면 안 됨            |
| **cpu**             | `requests.cpu`와 동일                                                          |
| **memory**          | `requests.memory`와 동일                                                       |
| **hugepages-**      | Huge Page라는 특수 메모리 리소스 사용량 제한                                   |

### 확장된 리소스쿼터

GPU 같은 특수 하드웨어 리소스도 쿼터로 제한 가능

- Overcommit(실제 물리 자원보다 더 많이 할당하는 것) 불가능: 같은 리소스를 나눠쓸 수 없음
  → Limit과 Request가 항상 같아야 함

## 스토리지 리소스쿼터

지정된 네임스페이스에서 요청할 수 있는 총 스토리지 리소스 합을 제한하는 기능

| 리소스 이름                                                             | 설명                                                                                                                        |
| ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| requests.storage                                                        | 모든 퍼시스턴트 볼륨 클레임에서 스토리지 요청의 합은 이 값을 초과할 수 없음                                                 |
| persistentvolumeclaims                                                  | 네임스페이스에 존재할 수 있는 총 퍼시스턴트 볼륨 클레임 수                                                                  |
| <storage-class-name>.storageclass.storage.k8s.io/requests.storage       | storage-class-name과 관련된 모든 퍼시스턴트 볼륨 클레임에서 스토리지 요청의 합은 이 값을 초과할 수 없음                     |
| <storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims | `<storage-class-name>` 과 관련된 모든 퍼시스턴트 볼륨 클레임에서 네임스페이스에 존재할 수 있는 총 퍼시스턴트 볼륨 클레임 수 |

로컬 임시 스토리지에 대한 쿼터 지원 기능이 추가

| 리소스 이름                | 설명                                                                                |
| -------------------------- | ----------------------------------------------------------------------------------- |
| requests.ephemeral-storage | 네임스페이스의 모든 파드에서 로컬 임시 스토리지 요청의 합은 이 값을 초과할 수 없음. |
| limits.ephemeral-storage   | 네임스페이스의 모든 파드에서 로컬 임시 스토리지 제한의 합은 이 값을 초과할 수 없음. |
| ephemeral-storage          | `requests.ephemeral-storage` 와 같음.                                               |

> **로컬 임시 스토리지**: pod나 컨테이너가 잠깐 데이터를 저장하기 위해 노드의 로컬 디스크 공간을 직접 사용하는 경우
>
> - Pod가 실행 중일 때만 존재하는 임시 저장공간

- example
  gold, bronze 각 스토리지 클래스 별로 쿼터를 지정하려는 경우
  ```
  gold.storageclass.storage.k8s.io/requests.storage: 500Gi
  bronze.storageclass.storage.k8s.io/requests.storage: 100Gi
  ```

## 오브젝트 수 쿼터

네임스페이스 안에서 특정 오브젝트를 몇 개까지 만들 수 있는지 개수 자체를 제한하는 쿼터

- 코어 그룹: count/<resource>
- 비코어 그룹: count/<resource>.<apiGroup>

> 코어 그룹: 가장 기본적인 리소스들
>
> Pod, Service, ConfigMap, Secret, PVC, Namespace, Node 등
>
> 비코어 그룹: 코어 그룹 외의 확장된 기능 리소스들

- 사용자 정의 리소스도 동일한 구문으로 적용 가능
  `widgets.example.com`과 같은 리소스에도 똑같이 개수 제한 가능
- 실행 종료 여부에 관계없이, etcd라는 내부 스토리지에 저장되어 있으면 계산
  → 저장공간이 꽉 차는 것을 막기 위한 방법

### 일반 오브젝트 수 쿼터

자주 쓰이는 리소스에 대해 count/를 생략하는 쿼터 이름도 제공 → 접두사 없이 쿼터로 제한 가능

| 리소스 이름            | 설명                                                                                                                                                                                                                        |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| configmaps             | 네임스페이스에 존재할 수 있는 총 컨피그맵 수                                                                                                                                                                                |
| persistentvolumeclaims | 네임스페이스에 존재할 수 있는 총 [퍼시스턴트 볼륨 클레임](https://kubernetes.io/ko/docs/concepts/storage/persistent-volumes/#%ED%8D%BC%EC%8B%9C%EC%8A%A4%ED%84%B4%ED%8A%B8%EB%B3%BC%EB%A5%A8%ED%81%B4%EB%A0%88%EC%9E%84) 수 |
| pods                   | 네임스페이스에 존재할 수 있는 터미널이 아닌 상태의 파드의 총 수. `.status.phase in (Failed, Succeeded)`가 true인 경우 파드는 터미널 상태임                                                                                  |
| replicationcontrollers | 네임스페이스에 존재할 수 있는 총 레플리케이션컨트롤러 수                                                                                                                                                                    |
| resourcequotas         | 네임스페이스에 존재할 수 있는 총 리소스쿼터 수                                                                                                                                                                              |
| services               | 네임스페이스에 존재할 수 있는 총 서비스 수                                                                                                                                                                                  |
| services.loadbalancers | 네임스페이스에 존재할 수 있는 `LoadBalancer` 유형의 총 서비스 수                                                                                                                                                            |
| services.nodeports     | 네임스페이스에 존재할 수 있는 `NodePort` 유형의 총 서비스 수                                                                                                                                                                |
| secrets                | 네임스페이스에 존재할 수 있는 총 시크릿 수                                                                                                                                                                                  |

## 쿼터 범위

특정 상태(조건)에 따라 적용 대상을 한정하는 기능

| 범위                      | 설명                                                                                                                                       | 추적 리소스 종류                                                            | 가능한 operator |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | --------------- |
| Terminating               | `.spec.activeDeadlineSeconds >= 0`에 일치하는 파드                                                                                         | pods, cpu, memory, requests.cpu, requests.memory, limits.cpu, limits.memory | Exists          |
| NotTerminating            | `.spec.activeDeadlineSeconds is nil`에 일치하는 파드                                                                                       |                                                                             | Exists          |
| NotBestEffort             | 서비스 품질이 나쁜 파드                                                                                                                    |                                                                             | Exists          |
| PriorityClass             | 지정된 [프라이어리티클래스](https://kubernetes.io/ko/docs/concepts/scheduling-eviction/pod-priority-preemption/)를 참조하여 일치하는 파드. |                                                                             | 모두            |
| CrossNamespacePodAffinity | 크로스-네임스페이스 파드 [(안티)어피니티 용어]가 있는 파드                                                                                 |                                                                             | 모두            |
| BestEffort                | 최상의 서비스 품질을 제공하는 파드                                                                                                         | BestEffort                                                                  | Exists          |

이 scope를 조합해 조건을 검사

- Terminating + NotTerminating, BestEffort + NotBestEffort → 불가능한 조합

### ScopeSelector

조건 기반으로 scope를 선택해 사용

```yaml
spec:
  scopeSelector:
    matchExpressions:
      - scopeName: PriorityClass
        operator: In
        values:
          - high
          - critical
```

## PriorityClass별 리소스 쿼터

특정 우선순위로 파드를 생성 가능

scopeSelector 필드를 사용해 pod의 우선순위에 따라 pod의 시스템 리소스 사용을 제어

- Example
  클러스터의 pod는 low, medium, high 세가지 우선 순위 클래스 중 하나를 가지고 각 우선 순위마다 하나의 쿼트 오브젝트 생성
  ```yaml
  apiVersion: v1
  kind: List
  items:
    - apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: pods-high
      spec:
        hard:
          cpu: "1000"
          memory: 200Gi
          pods: "10"
        scopeSelector:
          matchExpressions:
            - operator: In
              scopeName: PriorityClass
              values: ["high"]
    - apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: pods-medium
      spec:
        hard:
          cpu: "10"
          memory: 20Gi
          pods: "10"
        scopeSelector:
          matchExpressions:
            - operator: In
              scopeName: PriorityClass
              values: ["medium"]
    - apiVersion: v1
      kind: ResourceQuota
      metadata:
        name: pods-low
      spec:
        hard:
          cpu: "5"
          memory: 10Gi
          pods: "10"
        scopeSelector:
          matchExpressions:
            - operator: In
              scopeName: PriorityClass
              values: ["low"]
  ```

## 네임스페이스 간 파드 어피니티 쿼터

> Affinity: 파드가 같이 배치되고 싶은 관계
>
> AntiAffinity: 파드가 같이 배치되지 말아야 하는 관계

네임스페이스를 넘어서 파드 간 affinity를 설정하지 못하도록 제한하는 정책

- 다른 네임스페이스의 파드와도 affinity 관계를 맺는 것이 가능
  → 다른 네임스페이스까지 제약하기 때문에 리소스 경쟁이나 서비스 거부 상황 발생
- Example
  ```yaml
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: disable-cross-namespace-affinity
    namespace: foo-ns
  spec:
    hard:
      pods: "0"
    scopeSelector:
      matchExpressions:
        - scopeName: CrossNamespaceAffinity
  ```
  → foo-ns 네임스페이스 안에서는 CrossNamespaceAffinity 사용 불가능
  hard의 개수로 제한 가능
  ```yaml
  apiVersion: apiserver.config.k8s.io/v1
  kind: AdmissionConfiguration
  plugins:
    - name: "ResourceQuota"
      configuration:
        apiVersion: apiserver.config.k8s.io/v1
        kind: ResourceQuotaConfiguration
        limitedResources:
          - resource: pods
            matchScopes:
              - scopeName: CrossNamespaceAffinity
  ```
  → CrossNamespaceAffinity를 사용하는 파드의 경우 반드시 ResourceQuota로 허용된 네임스페이스에서만 만들 수 있음

## Request와 Limit

네임스페이스에 쿼터가 정의되어있으면 그 네임스페이스 안에서 만드는 모든 파드는 반드시 request 값을 적어야 함 → 네임스페이스 전체의 자원 사용 총량을 제한하기위한 쿼터 계산시 필요

## 쿼터 보기 및 설정

Kubectl에서 쿼터 생성, 업데이트 및 보기 지원

```
kubectl create namespace myspace
kubectl create -f ./compute-resources.yaml --namespace=myspace
kubectl create -f ./object-counts.yaml --namespace=myspace
kubectl get quota --namespace=myspace
kubectl describe quota compute-resources --namespace=myspace
kubectl describe quota object-counts --namespace=myspace
```

- `count/<resource>.<group>`을 통해 모든 표준 네임스페이스 리소스에 대한 오브젝트 수 쿼터를 지원
  ```
  kubectl create namespace myspace
  kubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace
  kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2
  kubectl describe quota --namespace=myspace
  ```

## 쿼터 및 클러스터 용량

리소스쿼터는 클러스터 용량과 무관

- 클러스터에 노드를 더 추가해도 네임스페이스의 쿼터는 자동적으로 제한되지 않음

→ 더 복잡한 정책이 필요

## 우선순위 클래스 소비 제한

→ 특정 PriorityClass 같은 논리적 리소스에도 제한을 걸 수 있음을 보여주는 예시

PriorityClass를 cluster-service(가장 높은 우선순위)를 쓰게되면 절대 종료되지 않는 파드가 새김 → cluster-service는 kube-system 네임스페이스 안에서만 쓰도록 제한

```yaml
apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
  - name: "ResourceQuota"
    configuration:
      apiVersion: apiserver.config.k8s.io/v1
      kind: ResourceQuotaConfiguration
      limitedResources:
        - resource: pods
          matchScopes:
            - scopeName: PriorityClass
              operator: In
              values: ["cluster-services"]
```

→ cluster-services라는 PriorityClass를 가진 파드 요청은, 반드시 ResourceQuota 검증을 거쳐야 함

- 결국 kube-system 내에서만 cluster-service를 쓰도록 제한
