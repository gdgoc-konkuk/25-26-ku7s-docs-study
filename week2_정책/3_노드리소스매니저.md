쿠버네티스가 고성능 워크로드(지연에 민감, 처리량이 큰 애플리케이션)를 효율적으로 실행하기 위해 제공하는 매니저

## Hardware topology alignment policies

CPU, 메모리, GPU 등의 하드웨어 자원을 물리적으로 가까운 위치에 정렬해 성능을 최적화하는 kubelet의 구성요소

- 정렬에 대한 정책 제공

## Policies for assigning CPUs to Pods

CPU Manager가 pod에 CPU를 어떻게 할당할지 결정하는 정책

- Multiplexing(공유): 여러 pod가 CPU 코어를 공유해서 사용
- Exclusive allocation(독접 할당): 특정 pod가 CPU 코어 몇개를 독점적으로 사용
- 왜?
  기본적으로 CFS(Completely Fair Scheduler)를 사용하는 이 구조에서는 CPU를 고정하지 않고 언제든 다른 코어로 이동할 수 있도록 함 → CPU migration
  실시간 처리, 머신러닝 추론, 네트워크 패킷 처리 등의 경우는 CPU migration 사용시 문제 발생
- 정책
  - None: 아무런 추가 제어 없이 CFS가 자동으로 CPU 배분
  - Static: 정수 단위의 CPU 요청을 가진 경우, 독점적으로 할당

### Static policy

특정 pod에게 CPU를 독점적으로 할당해주는 모드

- 프로세스

  1. 모든 CPU 코어를 shared pool에 추가
  2. kubelet이 시스템용으로 예약한 CPU들을 독점적으로 배정

     ```yaml
     --systemm-reserved=cpu=1
     --kube-reserved=cpu=1
     ```

     작은 core ID 순으로 배정

  3. BestEffort/Brustable 파드, Guaranteed 파드 중에서 정수가 아닌 CPU 요청 → 공유 풀에서 배정
  4. Guaranted pod & 정수 요청 CPU → 독점 배정

- 조건
  - CPU 예약은 반드시 > 0 → shared pool의 CPU 개수
  - CFS quota(CPU 시간 제한) 비활성화
  - 런타임 중 CPU 추가/삭제 불가능
- Example
  ```yaml
  spec:
    containers:
      - name: nginx
        image: nginx
  ```
  이러한 pod는 request와 limits가 정의되어 있지 않기 때문에 BestEfort이고 shared pool에서 동작
  ```yaml
  spec:
    containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "200Mi"
          requests:
            memory: "100Mi"
  ```
  위 pod는 request와 limits의 값이 다르기 때문에 Burstable이고 shared pool에서 동작
  ```yaml
  spec:
    containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "200Mi"
            cpu: "2"
          requests:
            memory: "200Mi"
            cpu: "2"
  ```
  request와 limit이 같기 때문에 Guaranteed이고 cpu 요청이 정수이기 때문에 2개의 CPU를 독점적으로 가짐
  ```yaml
  spec:
    containers:
      - name: nginx
        image: nginx
        resources:
          limits:
            memory: "200Mi"
            cpu: "1.5"
          requests:
            memory: "200Mi"
            cpu: "1.5"
  ```
  request와 limit이 같기 때문에 Guaranteed이지만 cpu 요청이 정수가 아니기 때문에 shared pool에서 동작

### Static policy options

각 옵션을 통해 CPU를 어떤 물리적 기준으로 묶거나 분산시킬지 제어

| 옵션 이름                            | 핵심 설명                                                                                 | 예시 상황                                                        |
| ------------------------------------ | ----------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **strict-cpu-reservation**           | 예약된 CPU(core) 에는 어떤 파드도 실행하지 않게 함.QoS 클래스에 상관없이 모두 차단.       | kubelet/systemd용으로 예약된 코어를 완전히 시스템 전용으로 보장. |
| **prefer-align-cpus-by-uncorecache** | CPU를 LLC(Last Level Cache, uncore cache) 경계에 맞춰 배치하려고 시도. (Best-effort 방식) | 캐시 공유 영역을 고려해서 CPU affinity를 높이고 싶을 때.         |

- full-pcpus-only
  pod가 요청한 CPU 수만큼 완전한 물리 코어를 배정해야 함
- distribute-cpus-across-numa
  CPU를 여러 NUMA 노드에 고르게 분산해 각 NUMA 노드에 비슷한 수의 CPU가 배정되도록 함
  > **NUMA(Non-Uniform Memory Access)**: CPU가 어떤 메모리에 접근하느냐에 따라 속도가 달라지는 구조 → CPU마다 접근하기 쉬운 메모리가 따로 있는 구조
- align-by-socket
  CPU를 socket 기준으로 묶어서 배치되도록 함
  > **CPU socket**: 물리적으로 CPU 칩이 꽂힌 자리
- distribute-cpus-across-cores
  CPUManager가 가상 코어를 가능한 한 다른 물리 코어에 고르게 분산 배치
- strict-cpu-reservation
  OS와 Kubernetes의 시스템 데몬이 사용하는 CPU 코어를 예약해두는 설정
- prefer-align-cpus-by-uncorecache
  CPUManager는 가능한 한 같은 LLC(uncore cache)를 공유하는 코어들만 한 컨테이너에 배정
  > **LLC(Last Level Cache)**: 메모리 바로 앞 캐시 레벨로 주로 여러 코어가 공유

## Memory Management Policies

Guaranteed Qos 클래스의 pod에 대해 메모리를 보장된 방식으로 할당하는 기능

- 목적
  파드가 요청한 메모리를 최소한의 NUMA 노드에서만 가져오도록 하기 위함
- 프로세스
  1. NUMA 노드 간의 메모리 위치를 고려해 가장 적합한 NUMA affinity(메모리-CPU 위치 관계)를 계산
  2. 이 위치를 topology manager에게 전달
  3. topology manager가 pod를 해당 노드에 배치할지 거부할지 결정
